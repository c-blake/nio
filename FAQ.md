### 0 - Hey!  This FAQ is more like a design document than "frequent" questions.

That's not a question. ;-)  Also, please just forgive the poetic license.

### 1 - What is NIO?

"Nio" are [two wrath-filled & muscular but "benevolent" King
guardians](https://en.wikipedia.org/wiki/Nio) of the Buddha, outside many
Buddhist temples.  Homonymic "Neo" is the messiah in The Matrix trilogy and a
Nim linear algebra package. ;)  (And you can pronounce it like "eye-oh" if you
prefer).

This NIO is a "Native/Numerical IO" system.  It consists of a library and a set
of command-line tools designed to standardize & simplify manipulation of simple
arrays of structs stored in files.  One dimensional/vector layouts are like
structs of arrays while rank3 and above also enjoy some support (eg. time series
of matrices).

The basic idea is to have files be self-describing with zero cost "parsing".
I.e. mmap & go or read a row at a time into one fixed buffer where packed field
access is mediated by optimizing compilers.  In addition to this zero overhead
access, NIO libraries also provide very low cost converting reads that can
translate missing value/NA codes.  This allows heterogeneous rows to be read
into heterogeneous program variables.  Data may not fit in memory and being able
to mmap & go lets one be "fluid" about data/program lifetime & computations,
subdividing however is convenient.

#### 1a - Why not bio for binary IO?

To avoid confusion with block/buffered IO which is "similar but different" in
the same context and focus on the main points of nativeness/number-hood.  Also,
"BIO" would introduce at least one pronunciation alternative that sounds like
"B.O." which in English speaking cultures abbreviates body odor and we try to
avoid "name smell" as well as "code smell". Lolz. ;-)

### 2 - How can it be "programming language agnostic" if it uses C type names?

Dependence upon C is only mnemonic.  C's ubiquity at the system level means
most/almost all prog.langs have exposure to C, e.g. for foreign function
interfaces.  A great many programmers who would never call themselves "C
programmers" nevertheless find the suffix syntax easy to remember.  I have
accessed NIO files from C, C++, Python, and Nim.  The format is all the same.

### 3 - NIO isn't CPU architecture neutral?  Aaaawhaaaaa?

Architecture neutrality was always over-rated for files for data analysis or
programmatic interaction, the main use case of NIO.  Neutrality has become ever
more outmoded since the early 2000s as the Intel-ARM hegemony has grown ever
more dominant.  To preserve mmap&go capability, one would need at least 2
copies, possibly made with some new `nio bswap` tool.  `dd swab` may be enough
for simple row formats.  It is possible to have the nio.(read|write) interfaces
do any needed byte swaps, at some IO performance hit.  I haven't personally
needed this capability in decades using these ideas, though I am not averse to
some PR for it.

While rare today, if you really have a cluster of heterogeneously endian
machines all computing against shared data and you cannot store two copies then
NIO may not add much value over other approaches since it loses full optimizing
compiler mediated access, though it may still be "simpler".  Nothing can be all
things to all folks in all circumstances.  As they say, your mileage may vary,
but those are many "ands" and perfect is the enemy of the good.

### 4 - Why is the type syntax so darn terse?  Why no file headers?

People use terse codes for outputs (like printf) all the time.  Why not for
inputs?  The input side is much simpler.  There is no base-10/16/.. variation,
no left/right adjustment or number of decimals or alternate formats.

One of the draws of the so-called Unix Philosophy is a simple consistent
newline-delimited row format, but text is particularly inefficient for numbers
(possibly << 1 GB/s vs >> 100 GB/s).  One way to view NIO is the simplest
possible generalization of this "simple, consistent format" to binary numeric
formats.  To me, simplest implies no headers since there are various ways you
can get ahold of an IO buffer, but this is admittedly somewhat subjective.

When used for shell pipelines prototyped interactively a terse syntax that does
not require shell quoting is helpful.  The utility of such pipelines also makes
users confront & thus quickly learn/memorize row format syntax.  Row format
transformations (such as combining columns) render explicitly.  Being explicit
has pros as well as cons & we won't settle that debate here.

### 5 - What about case-insensitive file systems?

I never use these myself and fail to see the appeal, but there are (at least)
two kinds of case-insensitivity: A) fully forgetful and B) store/present with
case variation but match insensitively.  Since it is already a type violation to
alter a row format after creation, and since users present pathnames often
generated by case-preserving operations, case B may cause little trouble.  For
case A) (and any case B gotcha/aesthetics), when there is no .N filename
extension, NIO tools will look for "dot files" of the same name but with a
leading '.'.  These can contain a string just like the filename extension.  For
example, "dateFoo" might have ".dateFoo" with contents "if@dates%s%.3f".  Then
users can simply say `nio p dateFoo` or otherwise `nOpen("dateFoo")`.  Note the
text after `[@%]` is really only special `nio print` syntax and ignored in other
contexts.  The file could also contain just "if" for most purposes.

### 6 - What do you mean "NIO formalizes/generalizes existing practice"?

Unix /var/run/utmp & /var/log/wtmp have had this format for decades.  This sort
of works as a poor man's utmpdump/last:
```
ln -s (/var/run/utmp|/var/log/wtmp) \
  typePidLineIdUserHostExitSessTvIp6rsv.Nsi32C4C32C256C2sl2l4i10C
nio p typePid*
# or on BSD approx typePadTvIdPadPidUserLineHostRsv.Ns6c2l8C4ci32C16C128C64C
```
Python's NumPy has had a `save` method to do this from the very beginning..
simply without a corresponding "reshape" metadata on load or mmap.  NYSE TAQ
data also used to ship .BIN files that were back-to-back structs.  I believe the
2010s saw option exchange data feeds move to this.  Similarly, a 128xM
24-bit/3-byte color image raster file can literally just be a binary file
"foo.N128,3c".  Standard numerical code using this access library could then
just access the raster or similar variants directly.

So, the general notion at play here is in use and has been for decades, but its
use is clunky/ad hoc requiring C FFIs, Perl/Python "pack" modules.  One standard
such as the NIO suffix format suffices to write *general* tools that can handle
any layout, transformation, multiple OSes, etc. as well as avoiding mucking bout
with `hexdump`, `od`, etc.

### 7 - Why not Pandas/R dataframes/Excel/JSON/XML/etc.?

The culture in this space is not to run directly off of stored files, but to
re-parse them constantly.  While they may be better than, well nothing at all,
this is a fundamentally expensive way to go and works well only for very small
data.  They often are like re-making a filesystem in program memory.  Why not
just use the existing FS?  Why not fully "save the answer" so almost no re-doing
of work is needed?  Data analysis is usually not like "emacs" where you can have
some very long running program loaded up in a costly way just once.  Imposing
such restrictions to acquire performance seems quite unnatural.  As explained
in the readme these costs can be factors of 100s..1000s.  Such ratios may well
be the diference between "fitting" on one fast server vs needing a big cluster.

### 8 - Why not a relational database like SQLite/MySql/etc.?

NIO is for use by programmer data analysts..perhaps advanced programmers who
think they can IO optimize better than query analyzers or who have custom
analytics or other needs to integrate with "real" prog.lang libraries that is
all too painful in SQL/SQL stored procedures (the latter of which are often very
non-DB portable).  Last I checked, it was a trick to even get Gaussian deviates
out of PostGres.  Trying to build some machine learning algo as part of a stored
procedure sounds like a nightmare.  Repayment for low-levelness is true zero
overhead IO (and easy access to SIMD speeds).  Updates are also often rare to
never; Yet analyses can hit large data sets 100s if not 100s of thousands of
times.  So, vectors/tables/tensors are apt while ACID transactions are over-
engineered.  Any cost is waste.  There may be a way to get mostly what you want
IO-wise from LMDB but specifying structure of the data will still need something
like NIO anyway.  In short, there seems definite value to non-DB persistence
formats.  The closest analogue to envisioned NIO use cases is HDF5.

### 9 - Ok..Why not HDF5?

Files & directories are a done deal.  HDF5 heralds from NetCDF & earlier formats
designed to work with very limited OS FSes of the 1970s & 1980s..E.g. DOS 8.3
filenames/VMS limits.  Consequently, these formats re-create/duplicate archive
functions (like tar/cpio/zip) instead of just using the FS.  Using regular files
has many charms, not the least of which is that users all know without any more
instruction how to list, find & manage files & directories within the already
hierarchical file system.  This includes many & varied compression programs -
not only library access to codecs but programs like zstd xyz or lz4 pdq etc.
Functionality bundles *seem* nice, but can also be a limiting luxury trap, e.g.
to support compression libs, ACLs, rsync optimization, encryption, or all other
things provided for files & dirs.  Orthogonality/independence is good.

### 10 - Doesn't KDB/APL derivative xyz do this already?

Somewhat, but not fully.  For example, back around the turn of the 2010
decade one could use `plzip` or `pixz` to get multi-GB/s scale IO from
multi-threaded decompression via backing store IO >20x less.  Nothing like
this was at all available for kdb, though it has surely grown support for
more compression modes.  Like HDF5, kdb is "overly bundled" in its concept
and better factoring wins the day.  To do this with the NIO model is easy.
Similar comments probably apply to other efforts like Apache Parquet&Arrow.

As far as I can tell, NIO is alone in striving for a flexible column/vector
|matrix|tensor "store" that strives to just solve **just one simple problem**:
not parsing & re-parsing and running "live" right off the files, but solve
that problem as generally as is easy.  All that said, the NIO solution is *so*
simple that it seems not improbable *someone* else has devised a close analogue,
especially in a simplified variant, such as only column stores.

### 11 - Ok..Why not a full object graph?

This could be a good addition.  Generalizing how string repositories work to
allow more arbitrary pointers may not even be hard.  Always insert-at-end/
mark deleted with some kind of eventual GC may retain a mostly usable "run
right off of files" for broader use cases, but notably will need to mimic
GC'd types like `seq` in Nim and possibly a lot of GC machinery.  PRs like
this are welcome, but note that relDBs/HDF5/etc. have somehow been useful
for near a half century without this feature and object-relational mappings
are usually considered thorny.

### 12 - Why no bit fields/discriminated unions/even fancier things?

This is a good question.  Choosing the best least common denominator is hard.
".N3:i5:i" instead of ".NC" with some prohibition on prefix multipliers might
work as a syntax for bit fields.  Discriminated unions (Nim's "variants")
might be .N?C10C for a 10Byte union.  One problem is that it is much less
obvious what zip, rip, cut, and similar transforms mean in the presence of
either.  Obviousness is good, as are general tools.

Another problem is portability.  While available in C/C++/Nim/.., they may
well be unavailable in NumPy/Julia/R/.. This fanciness just slightly exceeds
the floor of what many PLs consider necessary.  If you are willing to give up
the general tools & portability then you may be able to retain other nice
aspects of NIO & handle these with a native typedef/object and some kind of
sizeof, losing only data file portability to other PLs (or maybe to other
compilers within the same PL).  At this point the value of language-external
format collapses.

{ `R` does not do `float32` - to use NIO, you need `double` everywhere.
Similar comments apply to `unsigned`.  So, some might say the type system is
already "a half-step too fancy", but no `float` is a rare choice (one that has
aged poorly with SIMD popularity) and support for `unsigned` is easy to both
do & understand. }

In any event, the way to handle these cases is probably just a barely
descriptive NIO format with higher-level interpretation, as with strings.
Sadly, the interpretation is fancier than "pointers are repo indices".
(Well, with discriminated unions you might just have however many files
with the concrete types.)

### 13 - What about filename limits, like \< 255 chars?

If you are packing that many fields into single rows then you (or some upstream
dependency you have) are almost certainly on the wrong track, if for no other
reason than IO bw and the extraordinary unlikelihood you need all those fields
in every table scan.  In any event, you can still use dot files.

### 14 - Why don't you just *always* do column IO?

Column stores became all the rage in the 2010s and it's true in 2002 when I
first started doing things like this they had charm (and still do) for some
situations.  That said, sometimes you always want pairs/triples or in general
tuples that do not data compress much more as a tuple than they do in columns.
Or you may not be compressing at all.  Spreading your IO requests over 4 files
could have Winchester disk seek risk or other inefficiencies.  "To zip or rip?"
ultimately depends on hardware deployment & data context.  Since there is no way
to always know such answers at abstraction-creation-time, it is better not to
decide ahead of time.

Various zips of files like this would be called "materialized views" in the
database world.  As with almost everything in NIO, they are "available but
manual" since we assume users can code & reason about their data processing/
analysis needs "when it matters" at large scales.  At large scales things can
take hours, days, or weeks and factors of 2-10x can make enormous usability
differences.  So, no compromise access can be critical.

### 15 - Why don't you just always do simple tensor IO like x.N10,10f?

This special case, like column IO, can be exactly what you want sometimes.
Other times it can be helpful to zip tensors with identifying tags or other
metadata..perhaps only transiently, but transiently is "enough" to need support
in the format.

### 16 - Isn't the "type system" barely worthy of the name?

Yes & no.  It's basically the CPU type system (sans less portable latterday SIMD
types) rather than a more sophisticated programming language type system.  How
much of a problem this is depends upon what you are doing.  The main use case
for NIO is when performance matters which means big data which almost always
means big loops with low complexity data.  Low complexity data is not usually
too hamstrung by weak types.

Also, adding a type tag is not a crazy application of the above question's zip.
If you want then you can pair up everything .NCCff for a pair of distinct floats
with, say, units of measure encoded as the short integers.  You just need a
higher level of the system to interpret or enforce the types.  It would be more
efficient to add this extra metadata just once not for each record..maybe as a
paired .Txyz file or as another row in the dotfile.

### 17 - Why so many string repository styles?

Because no one can really agree on what is convenient and text varies so much.
Length-prefixed is the most general autonomous 8-bit clean format, but is harder
to edit by hand.  Delimited is nice, but not 8-bit clean.  Fixed width is 8-bit
clean and even affords smaller integer row number indices, but then is fixed
width, meaning it has to truncate and takes up a lot of space.

There are actually (at least) two more unrepresented useful styles.  The first
is newline-delimited-but-line-number-indexed (rather than byte offset-indexed).
With this, you can just fire up a text editor and hack away on string defs with
no regard to keeping string lengths the same.  The downside is that, at load
time, you must parse the newlines in the file which can take time for large
repositories.  The second is back-to-back undelimited string data with external
length, index data.  This is as fast & general as length-prefixed, but is also
non-autonomous - external data is needed to identify string boundaries.

### 18 Why no variable width inline strings like protobufs/flatbuffers/etc.?

NIO embraces files & directories (and their context) which, again, is a done
deal rather than trying to make "autonomous buffers".  Fixed size records allow
easy random access and fast subset access is viewed as important..say take the
last 1% of rows or a randomized 1% subsample.  To preserve this w/inline VW
strings, you need (at least) a separate global index telling offsets of records
as well as a (probably) in-record index telling the start of fields (at least
after the first inline string).  While the latter could precede each row, that'd
be somewhat expensive in space (possibly doubling if average pointer size =~
average datum size).

If one shares some string repo across many fields then one can compare for
equality by pointer, sometimes called string interning.  This optimization falls
naturally out of the repository arrangement.  The indirection is potentially
costly, but for many analyses one would often prefer class numbers anyway
(another way to think of interning is like an "automatic enum") for all the
internal calculation only stringifying at the end.  So, the indirection is
helpful for many use cases, especially with indirect fixed-width strings where
it is cheap to make the pointer a row number that can be thought of like a file
descriptor or other "only as big as needed" label.

If one can ensure sort order on the repo all in-set string compares can even be
reduced to integer compares.  That is rather costly requiring either another
level of indirection (say via a B-tree which itself might have a level or three)
or a batch sort with a re-org of all the pointers which is only practical for
truly static data.  Since NIO is not a string-focused facility, this is unikely
to be popular feature anyway.

### 19 What about "schema versioning"/evolution?

Since files are autonomous, you can usually just add new columns as new files
with compatible indexing.  This gives you schema addition with no need to even
version and "duck type" checking of what is available via fileExists API calls.
This is especially easy if you are doing a pure-column files setup, but is also
fine with various tables.  You can even add columns in batches as new tables.

*Deleting* columns is not easy this way, but then again this also tends to be
a disaster.  In particular you need to ensure no code anywhere is using what
you are deleting.  That is enough work/risk that you should maybe just rename
the directory/type itself.

In any case, you could always just drop a `version` file in the directory if
you think some string can capture the needed labeling.  Note that semantics
are often not fully covered by types and can bit you, such as a differential
field becoming a cumulative field.  At some level you do need to understand
data that you calculate against.

### 20 - This is all hopelessly hard to use compared to SQL

Also not a question.  I think reasonable folks can differ on this and I am open
to usability suggestions.  Also, the idea is kind of "between" the IO parts of
DBs and the access/query parts.  So, you could think of it as a way to layer
building a DB in such a way that preserves no compromise access by programmers
willing to put in some effort.  E.g., query language-like functionality can be
layered on top, and more transactional ideas could be stuffed in underneath,
hopefully optionally to preserve efficiency.  NIO is just a supplementary point
in the design space rather than an outright replacement.  Not appealing to all
in all cases is just another way of saying "Yup.  It's software." ;)

### 21 - Ok.  I am *so* sold, BUT what about *my* programming language?

Hey..Glad you like the idea (and even read to the end).  There are only so many
hours in the day, though.  My hope is that the core idea is simple enough to be
replicated in any PL with any kind of low-level IO.  The core filename extension
parser, `nio.initIORow`, is like 25 lines of non-comment Nim.

The missing value|N/A convention is perhaps less critical in a first pass.  A
full suite of tools like loaders/parsers/printers/zip/rip is not necessary *if*
you are merely willing to compile/learn how to use the nio program here.

Once you have a basic access libraries you could write n-foo CL tools (or libs)
in a dozen PLs if you want..n-awk, n-plot, n-histo, etc.  As already mentioned,
the main use case is custom calculations over big data where all you really need
is the row stream/mmap interfaces and your own code which may not need little
generality.  OTOH, maybe there are dozens of general things to do.  Have at it.

NIO is so simple/easy that its main value to the world might not be *any* one
implementation, tool, or library, but merely some standard suffix syntax/naming
convention.  Stick to that convention and the world benefits from any/all impls
in any/all PLs, like CSV or TSV or whatever, but with no per-datum parsing.
